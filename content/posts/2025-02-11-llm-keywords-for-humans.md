title = "LLM Keywords for Humans"
date = 2025-02-11T18:23:00+02:00
tags = [
  "llm",
  "keywords",
  "humans",
  "context",
  "window",
  "parameters",
  "deepseek",
  "openai",
  "llama",
  "model"
]
published = true
+++++
With everyone discussing the world’s best AI models, it’s important to humanize the language we use to describe them. Understanding these key terms will help make conversations about large language models (LLMs) more accessible.

**Model Family**

Represents the brand of the model. There may be many models named similarly but they all belong to one "family". Like how Toyota has multiple cars. Llama has many types of models that belong to the Llama family.

**Parameters**

The more parameters a model has, the more detailed and complex it's understanding of it's training data. Thus having more detailed knowledge on individual topics which result in a more reliable and detailed answer.

**Context Window**

How much an AI can 'remember' at once while generating a response. A larger window means the LLM will have better memory when processing your next request... while taking context from previous requests into account.

### Example:

Take the [deepseek-r1](https://artificialanalysis.ai/models/deepseek-r1) model, which has been generating a lot of buzz. Its model family is “Deepseek,” and the specific model is “R1,” with multiple versions available.

On [Ollama](https://ollama.com/library/deepseek-r1) model details page it shows the `deepseek-r1` model as having the following parameter options: `1.5b`, `7b`, `8b`, `14b`, `32b`, `70b`, and `671b`.

 - The `1.5b` being the least knowledgable.
 - The `671b` model being the most knowledgable.

The `671b` model boasts a `128k` **context window**, meaning it can remember around fifty **1,000 to 2,000** word essays at once.

Hopefully, this helps distill AI terminology into simpler, more understandable concepts.
